%matplotlib inline
%config InlineBackend.figure_format='retina'
from urllib.request import urlretrieve
from os.path import isfile,isdir
from tqdm import tqdm
import tarfile
import random
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import OneHotEncoder
import pickle

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
import tensorflow as tf
from tensorflow.python.framework import ops
from PIL import Image

from tensorflow.keras.datasets import cifar10
(x_train,y_train), (x_test, y_test)=cifar10.load_data()

#Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
170500096/170498071 [==============================] - 93s 1us/step

x_train.shape
#(50000, 32, 32, 3)

x_train[0]
#array([[[ 59,  62,  63],
        [ 43,  46,  45],
        [ 50,  48,  43],
        ...,
        ...,
        [216, 184, 140],
        [151, 118,  84],
        [123,  92,  72]]], dtype=uint8)

x_train[0].shape
#(32, 32, 3)

plt.imshow(x_train[0])

x_train[0].max()
#255
#scale them
x_train=x_train/255
x_test=x_test/255

#Convert labels to categorical data
from tensorflow.keras.utils import to_categorical
y_cat_train=to_categorical(y_train,10)
y_cat_test=to_categorical(y_test,10)

#Build the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten

model=Sequential()

model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32,32,3),activation='relu',))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32,32,3),activation='relu',))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(256,activation='relu'))
model.add(Dense(10,activation='softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])


model.summary()

#Add early stopping
from tensorflow.keras.callbacks import EarlyStopping
early_stop=EarlyStopping(monitor='val_loss',patience=1)
model.fit(x_train,y_cat_train,epochs=10,validation_data=(x_test,y_cat_test),callbacks=[early_stop])
#poch 1/10
1563/1563 [==============================] - 48s 31ms/step - loss: 1.5337 - accuracy: 0.4447 - val_loss: 1.3967 - val_accuracy: 0.5091
Epoch 2/10
1563/1563 [==============================] - 60s 38ms/step - loss: 1.1720 - accuracy: 0.5842 - val_loss: 1.1024 - val_accuracy: 0.6130
Epoch 3/10
1563/1563 [==============================] - 57s 37ms/step - loss: 1.0146 - accuracy: 0.6429 - val_loss: 1.1022 - val_accuracy: 0.6202
Epoch 4/10
1563/1563 [==============================] - 55s 35ms/step - loss: 0.9048 - accuracy: 0.6831 - val_loss: 0.9876 - val_accuracy: 0.6529
Epoch 5/10
1563/1563 [==============================] - 63s 40ms/step - loss: 0.8149 - accuracy: 0.7161 - val_loss: 0.9713 - val_accuracy: 0.6652
Epoch 6/10
1563/1563 [==============================] - 63s 40ms/step - loss: 0.7463 - accuracy: 0.7392 - val_loss: 0.9507 - val_accuracy: 0.6767
Epoch 7/10
1563/1563 [==============================] - 67s 43ms/step - loss: 0.6847 - accuracy: 0.7615 - val_loss: 0.9491 - val_accuracy: 0.6835
Epoch 8/10
1563/1563 [==============================] - 71s 45ms/step - loss: 0.6241 - accuracy: 0.7816 - val_loss: 0.9638 - val_accuracy: 0.6792
<tensorflow.python.keras.callbacks.History at 0x18319d56b48>



#Evaluate the model
metrics=pd.DataFrame(model.history.history)
metrics

#loss	accuracy	val_loss	val_accuracy
0	1.533701	0.44466	1.396719	0.5091
1	1.172014	0.58424	1.102398	0.6130
2	1.014598	0.64290	1.102153	0.6202
3	0.904814	0.68306	0.987565	0.6529
4	0.814901	0.71608	0.971253	0.6652
5	0.746332	0.73922	0.950709	0.6767
6	0.684709	0.76150	0.949128	0.6835
7	0.624137	0.78156	0.963832	0.6792

metrics.plot()
#<matplotlib.axes._subplots.AxesSubplot at 0x22108b3f088>

metrics[['accuracy','val_accuracy']].plot()
#<matplotlib.axes._subplots.AxesSubplot at 0x1831b503308>

model.evaluate(x_test,y_cat_test)
#313/313 [==============================] - 2s 6ms/step - loss: 0.9638 - accuracy: 0.6792
[0.9638320207595825, 0.6791999936103821]

from sklearn.metrics import classification_report,confusion_matrix
predictions=model.predict_classes(x_test)
print(classification_report(y_test,predictions))
# precision    recall  f1-score   support

           0       0.73      0.71      0.72      1000
           1       0.74      0.81      0.77      1000
           2       0.63      0.49      0.55      1000
           3       0.50      0.45      0.47      1000
           4       0.63      0.65      0.64      1000
           5       0.54      0.64      0.58      1000
           6       0.75      0.78      0.76      1000
           7       0.74      0.73      0.74      1000
           8       0.79      0.78      0.79      1000
           9       0.75      0.75      0.75      1000

    accuracy                           0.68     10000
   macro avg       0.68      0.68      0.68     10000
weighted avg       0.68      0.68      0.68     10000



confusion_matrix(y_test,predictions)
#array([[712,  41,  43,  24,  28,  12,  12,  13,  74,  41],
       [ 20, 811,   9,  15,   2,   5,  14,   5,  28,  91],
       [ 63,  13, 488,  88, 104, 101,  86,  29,  17,  11],
       [ 30,  24,  55, 448,  72, 237,  56,  41,  16,  21],
       [ 27,   5,  60,  57, 650,  51,  46,  81,  14,   9],
       [ 16,   9,  43, 139,  50, 643,  24,  56,   8,  12],
       [  4,  14,  38,  53,  45,  48, 776,   8,   8,   6],
       [ 24,   7,  21,  40,  64,  76,  11, 732,   3,  22],
       [ 49,  61,   9,   9,  14,  14,  11,  11, 780,  42],
       [ 27, 115,  11,  24,   5,  12,   5,  14,  35, 752]], dtype=int64)

import seaborn as sns
plt.figure(figsize=(10,6))
sns.heatmap(confusion_matrix(y_test,predictions),annot=True)
#<matplotlib.axes._subplots.AxesSubplot at 0x1831d147cc8>

my_image=x_test[0]
plt.imshow(my_image)
y_test[0]
#array([3], dtype=uint8)

model.predict_classes(my_image.reshape(1,32,32,3))
#array([3], dtype=int64)

